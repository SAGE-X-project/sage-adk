# LLM Provider Design Document

**Version**: 1.0
**Created**: 2025-10-07
**Author**: SAGE ADK Development Team

## Overview

This document describes the design for LLM (Large Language Model) provider abstraction, which allows AI agents to interact with multiple LLM providers (OpenAI, Anthropic, Gemini) through a unified interface.

## Design Principles

Based on AI agent development research:
- **Provider Agnostic**: One interface, multiple providers
- **Progressive Disclosure**: Simple for basic use, powerful for advanced
- **Type Safety**: Strong typing for requests and responses
- **Error Handling**: Consistent error handling across providers
- **Streaming Support**: Built-in support for streaming responses

## Architecture

### LLM Provider Interface

```go
type Provider interface {
    Name() string
    Complete(ctx context.Context, req *CompletionRequest) (*CompletionResponse, error)
    Stream(ctx context.Context, req *CompletionRequest, fn StreamFunc) error
    SupportsStreaming() bool
}
```

### Provider Registry

```go
type Registry struct {
    providers map[string]Provider
    default   Provider
}
```

### Supported Providers

1. **OpenAI** (GPT-4, GPT-3.5-turbo)
2. **Anthropic** (Claude 3.5 Sonnet, Claude 3 Opus)
3. **Gemini** (Gemini Pro, Gemini Ultra)

## Implementation Strategy

### Phase 1: Core Abstraction

**Minimal implementation:**
- Provider interface definition
- Basic request/response types
- Mock provider for testing
- Registry for provider management

**NOT included in Phase 1:**
- Actual provider implementations (OpenAI, Anthropic, Gemini)
- Streaming support (deferred)
- Advanced features (function calling, vision, etc.)

### Phase 2: Provider Implementations

Implement actual LLM providers:
- OpenAI provider
- Anthropic provider
- Gemini provider

### Phase 3: Advanced Features

Add advanced capabilities:
- Streaming support
- Function calling
- Vision (multimodal)
- Token counting
- Cost estimation

## Type Definitions

### CompletionRequest

```go
type CompletionRequest struct {
    Model       string              // Model name
    Messages    []Message           // Conversation history
    MaxTokens   int                 // Maximum tokens to generate
    Temperature float64             // Sampling temperature
    TopP        float64             // Nucleus sampling
    Stream      bool                // Enable streaming
    Metadata    map[string]string   // Provider-specific metadata
}
```

### Message

```go
type Message struct {
    Role    MessageRole // "user", "assistant", "system"
    Content string      // Message content
}

type MessageRole string

const (
    RoleUser      MessageRole = "user"
    RoleAssistant MessageRole = "assistant"
    RoleSystem    MessageRole = "system"
)
```

### CompletionResponse

```go
type CompletionResponse struct {
    ID        string              // Response ID
    Model     string              // Model used
    Content   string              // Generated content
    FinishReason string           // Reason for completion
    Usage     *Usage              // Token usage
    Metadata  map[string]string   // Provider-specific metadata
}
```

### Usage

```go
type Usage struct {
    PromptTokens     int // Tokens in prompt
    CompletionTokens int // Tokens in completion
    TotalTokens      int // Total tokens used
}
```

## Configuration

```yaml
llm:
  provider: "openai"           # Default provider
  api_key: "sk-..."            # API key
  max_tokens: 2000             # Default max tokens
  temperature: 0.7             # Default temperature
  timeout: 30                  # Request timeout (seconds)
```

## Provider-Specific Configuration

### OpenAI
```yaml
llm:
  provider: "openai"
  api_key: "sk-..."
  model: "gpt-4"
  organization: "org-..."      # Optional
```

### Anthropic
```yaml
llm:
  provider: "anthropic"
  api_key: "sk-ant-..."
  model: "claude-3-5-sonnet-20241022"
  version: "2023-06-01"        # API version
```

### Gemini
```yaml
llm:
  provider: "gemini"
  api_key: "..."
  model: "gemini-pro"
  project: "my-project"        # GCP project
```

## Phase 1 Implementation

### Mock Provider

For testing and development:

```go
type MockProvider struct {
    name      string
    responses []string
    index     int
}

func (m *MockProvider) Complete(ctx context.Context, req *CompletionRequest) (*CompletionResponse, error) {
    if m.index >= len(m.responses) {
        return nil, errors.ErrLLMInvalidResponse.WithMessage("no more responses")
    }

    response := &CompletionResponse{
        ID:      "mock-" + uuid.New().String(),
        Model:   req.Model,
        Content: m.responses[m.index],
        FinishReason: "stop",
        Usage: &Usage{
            PromptTokens:     100,
            CompletionTokens: 50,
            TotalTokens:      150,
        },
    }

    m.index++
    return response, nil
}
```

### Registry Implementation

```go
type Registry struct {
    mu        sync.RWMutex
    providers map[string]Provider
    default   Provider
}

func NewRegistry() *Registry {
    return &Registry{
        providers: make(map[string]Provider),
    }
}

func (r *Registry) Register(name string, provider Provider) {
    r.mu.Lock()
    defer r.mu.Unlock()
    r.providers[name] = provider
}

func (r *Registry) Get(name string) (Provider, error) {
    r.mu.RLock()
    defer r.mu.RUnlock()

    provider, ok := r.providers[name]
    if !ok {
        return nil, errors.ErrNotFound.WithDetail("provider", name)
    }
    return provider, nil
}

func (r *Registry) SetDefault(provider Provider) {
    r.mu.Lock()
    defer r.mu.Unlock()
    r.default = provider
}

func (r *Registry) Default() Provider {
    r.mu.RLock()
    defer r.mu.RUnlock()
    return r.default
}
```

## Integration with Agent

Agents can use LLM providers:

```go
agent, _ := NewAgent("llm-agent").
    OnMessage(func(ctx context.Context, msg MessageContext) error {
        // Get LLM provider
        llm := getLLMProvider()

        // Create completion request
        req := &llm.CompletionRequest{
            Model: "gpt-4",
            Messages: []llm.Message{
                {Role: llm.RoleUser, Content: msg.Text()},
            },
            MaxTokens: 1000,
        }

        // Get completion
        resp, err := llm.Complete(ctx, req)
        if err != nil {
            return err
        }

        // Reply with LLM response
        return msg.Reply(resp.Content)
    }).
    Build()
```

## Error Handling

Map provider errors to SAGE ADK errors:

```go
func convertLLMError(err error) error {
    if err == nil {
        return nil
    }

    // Map specific errors
    if strings.Contains(err.Error(), "rate limit") {
        return errors.ErrLLMRateLimit.Wrap(err)
    }

    if strings.Contains(err.Error(), "timeout") {
        return errors.ErrLLMTimeout.Wrap(err)
    }

    if strings.Contains(err.Error(), "invalid") {
        return errors.ErrLLMInvalidResponse.Wrap(err)
    }

    // Default to connection error
    return errors.ErrLLMConnection.Wrap(err)
}
```

## Testing Strategy

1. **Mock Provider Tests**: Test registry and interface
2. **Unit Tests**: Test each provider implementation (Phase 2)
3. **Integration Tests**: Test with actual LLM APIs (manual, not CI)

## Minimal Phase 1 Scope

For initial implementation:

1. ✅ **Interface Definition**: Provider interface
2. ✅ **Types**: Request/Response types
3. ✅ **Registry**: Provider registration and lookup
4. ✅ **Mock Provider**: For testing
5. ❌ **Real Providers**: Deferred to Phase 2
6. ❌ **Streaming**: Deferred to Phase 3

This allows:
- Agent integration testing
- Protocol layer testing
- Framework structure completion
- Defer expensive API implementation

## Dependencies

Phase 1: None (mock only)

Phase 2:
```go
require (
    github.com/sashabaranov/go-openai v1.x.x      // OpenAI
    github.com/anthropics/anthropic-sdk-go v0.x.x  // Anthropic
    github.com/google/generative-ai-go v0.x.x      // Gemini
)
```

## Future Enhancements

1. **Streaming**: Real-time token streaming
2. **Function Calling**: Tool/function invocation
3. **Vision**: Image understanding
4. **Embeddings**: Vector embeddings generation
5. **Fine-tuning**: Custom model training
6. **Caching**: Response caching for efficiency
7. **Retry Logic**: Automatic retries with backoff
8. **Rate Limiting**: Client-side rate limiting

## References

- OpenAI API: https://platform.openai.com/docs/api-reference
- Anthropic API: https://docs.anthropic.com/claude/reference
- Gemini API: https://ai.google.dev/docs
- AI Agent Research: `docs/analysis/` directory
